{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8470fabd",
   "metadata": {},
   "source": [
    "# Getting Data from the Web\n",
    "\n",
    "This notebook introduces ways to collect data from the web using Python. You'll learn how to fetch data from APIs, explore JSON, and (optionally) scrape simple web pages. These skills can help you automate the process of gathering open data for your research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ca1f2",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "Automating data collection saves time and reduces errors compared to downloading files by hand. Common research uses include:\n",
    "- Downloading weather or environmental data\n",
    "- Collecting census or population statistics\n",
    "- Gathering real estate or planning datasets\n",
    "\n",
    "**APIs** (Application Programming Interfaces) are official ways to get data directly from a website or service, often in a structured format like JSON. **Web scraping** means extracting data from the visible content of web pages, which is less reliable and may not be allowed by the website's terms.\n",
    "\n",
    "> **Tip:** Always check a website's terms of use and robots.txt before scraping. Use APIs when available, respect rate limits, and avoid overloading servers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d406df08",
   "metadata": {},
   "source": [
    "## 2. Accessing an API with `requests`\n",
    "\n",
    "Let's use the `requests` library to fetch data from a public API. We'll use [JSONPlaceholder](https://jsonplaceholder.typicode.com/) as a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e760501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Fetch a list of sample users\n",
    "url = \"https://jsonplaceholder.typicode.com/users\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Always check the status code first\n",
    "print(\"Status code:\", response.status_code)\n",
    "\n",
    "# Print a preview of the raw response text\n",
    "print(response.text[:200])  # Show only the first 200 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1806ace",
   "metadata": {},
   "source": [
    "## 3. Exploring JSON Responses\n",
    "\n",
    "Most APIs return data in **JSON** format (like a nested dictionary/list). Let's convert the response and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844e5372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the response to JSON (Python objects)\n",
    "data = response.json()\n",
    "\n",
    "print(type(data))  # Should be a list\n",
    "print(len(data), \"records\")\n",
    "\n",
    "# Look at the first record\n",
    "print(data[0])\n",
    "\n",
    "# Print the available keys in the first record\n",
    "print(list(data[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545e63ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and print a few fields from each user\n",
    "for user in data[:3]:  # Just show first 3 users\n",
    "    print(\"Name:\", user['name'], \"| Email:\", user['email'], \"| City:\", user['address']['city'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dff457",
   "metadata": {},
   "source": [
    "## 4. From JSON to DataFrame\n",
    "\n",
    "You can load JSON data into a pandas DataFrame for analysis. If the data is nested, `pd.json_normalize()` helps flatten it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46118c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Flatten nested fields like address.city\n",
    "df = pd.json_normalize(data, sep='_')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e80c70",
   "metadata": {},
   "source": [
    "## 5. Optional: Scraping a Simple Web Page\n",
    "\n",
    "If no API is available, you might need to scrape data from a web page. Let's use `requests` and `BeautifulSoup` to extract headlines from a simple website.\n",
    "\n",
    "> **Ethics & Legality:**\n",
    "> - Always check the website's robots.txt and terms of service.\n",
    "> - Don't overload servers—add delays if scraping many pages.\n",
    "> - Attribute your sources and respect data ownership.\n",
    "> - Some sites may block or ban scrapers.\n",
    "\n",
    "Below is a basic example (scraping Wikipedia headlines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107a9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Architecture\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find all the section headings (h2 tags)\n",
    "headings = soup.find_all('h2')\n",
    "for h in headings[:5]:  # Show first 5 headings\n",
    "    print(h.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21458ce",
   "metadata": {},
   "source": [
    "## 6. Saving and Reusing Fetched Data\n",
    "\n",
    "It's a good idea to save the raw data you fetch, so you can reuse it or check your work later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4795cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save JSON data to a file\n",
    "with open('users_data.json', 'w') as f:\n",
    "    json.dump(data, f, indent=2)\n",
    "\n",
    "# Save DataFrame to CSV\n",
    "df.to_csv('users_data.csv', index=False)\n",
    "print(\"Data saved to users_data.json and users_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e8e1f",
   "metadata": {},
   "source": [
    "## 7. Practice Challenge (Optional)\n",
    "\n",
    "**Task:**\n",
    "- Use an API to get data about a place (e.g. city weather, population, or similar).\n",
    "- Extract 5 fields from the response.\n",
    "- Convert the result into a CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970647b5",
   "metadata": {},
   "source": [
    "## 8. Summary & Links\n",
    "\n",
    "- Use APIs when possible—they're more reliable and ethical than scraping.\n",
    "- Scraping should be a last resort, and always done with care and respect for the source.\n",
    "- Store your raw data before transforming it.\n",
    "\n",
    "### Useful Links\n",
    "- [List of public APIs](https://github.com/public-apis/public-apis)\n",
    "- [pandas documentation](https://pandas.pydata.org/docs/)\n",
    "- [requests documentation](https://requests.readthedocs.io/en/latest/)\n",
    "- [BeautifulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "\n",
    "Next up: How to structure and share your own Python projects!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
